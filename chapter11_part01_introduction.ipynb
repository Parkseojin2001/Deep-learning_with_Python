{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4jYv6rlhoZV3sbo24TKYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parkseojin2001/Deep-learning_with_Python/blob/main/chapter11_part01_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.2 텍스트 데이터 준비"
      ],
      "metadata": {
        "id": "x5_1PohvuT5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2.4 TextVectorization 층 사용하기"
      ],
      "metadata": {
        "id": "1m49j2EYuYoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "WTuAEn8Tujba"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hLfXPSk0uLb0"
      },
      "outputs": [],
      "source": [
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]"
      ],
      "metadata": {
        "id": "ezj7YiazvjuX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.make_vocabulary(dataset)"
      ],
      "metadata": {
        "id": "iiWryStFwCni"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)"
      ],
      "metadata": {
        "id": "TmSOdrPvwX68"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg_jAPii0UzJ",
        "outputId": "9b2530a6-12fb-489c-80e4-e9b8ff686b04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)"
      ],
      "metadata": {
        "id": "TFRycOAi0V7-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgF6RRFU1qNh",
        "outputId": "4a4275f4-7288-47e8-e7ba-cb5f71488da9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TextVectorization 층 사용\n",
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",    # 정수 인덱스로 인코딩된 단어 시퀀스를 반환하도록 설정\n",
        ")"
      ],
      "metadata": {
        "id": "Oa16VQsi1v0P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "  lowercase_string = tf.strings.lower(string_tensor)  # 문자열을 소문자로 변경\n",
        "  return tf.strings.regex_replace(  # 구두점 문자를 빈 문자열로 변경\n",
        "      lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "  return tf.strings.split(string_tensor)  # 공백을 기준으로 문자열 나누기\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "0DdZML1w3eU1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "ntTmrXaG5AS4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어휘 사전 출력하기\n",
        "text_vectorization.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiH4_Rsn6W6Q",
        "outputId": "9fe8b5bb-9d44-4cde-c85f-92c027939af0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 문장 인코딩\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXxne3y89Egk",
        "outputId": "987950ac-b9e8-4782-e09b-bce8346741bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 문장 디코딩\n",
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m60LEAgH9c2a",
        "outputId": "867a7dc1-0933-4de2-8b2b-d0e86f65e316"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.3 단어 그룹을 표현하는 두 가지 방법: 집합과 시퀀스"
      ],
      "metadata": {
        "id": "oJ3R7N9GEU2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMDB 영화 리뷰 데이터 준비하기"
      ],
      "metadata": {
        "id": "QwWBT3FlEb9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55GmsRDUETSE",
        "outputId": "4ccbf197-bd6e-4536-eef8-5de49dfcda34"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  5415k      0  0:00:15  0:00:15 --:--:-- 13.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "Z0TwWKV3EhQg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhoSoqDaEi4k",
        "outputId": "2ff06b5b-6f28-4cbf-df1e-73925a8319f7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)    # 코드를 여러 번 실행해도 동일한 검증 세트를 생성\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]  # 훈련 파일 중 20%를 검증 세트로 이용\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)   # 파일 위치 이동"
      ],
      "metadata": {
        "id": "ewD8L5UCEkNL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d_UsFT3EohP",
        "outputId": "b3785084-6b83-47e4-87ef-fa1c274a5b26"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 배치의 크기와 dtype 출력하기\n",
        "\n",
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRSd5IirEv2w",
        "outputId": "5c54a809-69f3-498e-ec42-3e2a56df7f31"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'When I saw this movie I heard all the hype, and I heard how people said that Denzel deserved the Oscar alongside his Golden Globe and I believed he must have done an outstanding job considering Kevin Spacey was excellent. I was wrong. I realize that people say this not to anger the African American community (if they are not African Americans themselves). I always hear complaints on how African Americans are never nominated and how they should have won. Sometimes this is true (not as much nowadays) because Whoppi Goldberg should have won best actress for The Color Purple and the movie should have won best picture. The only reason this movie was so blown up the way it was, is because people see a movie about the (*SEMI-SPOILER*) hardships of an African American during a very racist time period and they automatically label it as a masterpiece.<br /><br />Denzel Washington is an outstanding actor, but his role in this movie did not affect me whatsoever. I was bored with him in the movie, and his acting here was quite similar to his role in Malcolm X but not as good. The audience is supposed to leave believing this man, Rubin Carter, is a saint. People left the movies worshipping this man, this hero, and they went out and bought his book, making this hero of a man rich.<br /><br />*SPOILERS* This movie tells the tale of a man who spent the majority of his life in prison mainly for crimes he did not commit. Of course the crimes he did commit (stealing mostly) was only to survive, nothing more. People felt sorry for him, even though the drug dealers and thieves probably amounted to as much for the same reasons but are looked down upon in society. Everything in this movie tries to portray this man as a saint (except for the obvious infidelity he had towards his wife and the aggression he showed the other man when he met his wife) but why wouldn\\'t it-after all, it is his point of view. I do not like movies (especially Hollywood interpretations) that are based on \"true\" stories because they usually distort the \"true\" parts into something else, something not so true. This was his point of view and a Hollywood construction, yet everyone believed it was the truth unquestionably. Well I researched his past a bit before making any assumptions, and he was a very violent man. Not only that, there is still a possibility that he did murder those people. If you do not believe me, search for him on the internet, and read the articles some people have of him. The boxing match he claimed to have won so easily, was actually won by his opponent Joey Giardello and there are tapes to prove it. Besides that, there are many twisted and purposely left out facts in this movie. The supporting cast were the nicest people I have ever seen on the face of the Earth and their \"nice and perfect\" persona looked difficult to keep up.<br /><br />This movie was a Hollywood version of yet another unfortunate true story that is still left to be told truthfully. Denzel\\'s acting is stale, and the supporting cast\\'s Mickey Mouse attitudes are annoying. The movie also begins very slow paced and is boring.', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3.2 단어를 집합으로 처리하기: BoW 방식"
      ],
      "metadata": {
        "id": "Gwzm0ND1FLa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 이진 인코딩을 사용한 유니그램"
      ],
      "metadata": {
        "id": "aT-jbQisSrRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TextVectorization 층으로 데이터 전처리하기\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,   # 가장 많이 등장하는 2만 개 단어로 어휘 사전 제한\n",
        "    output_mode=\"multi_hot\",  # 멀티-핫 이진 벡터로 출력 토큰 인코딩\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)   # 원시 텍스트 입력만 반환\n",
        "text_vectorization.adapt(text_only_train_ds) # adapt() 메서드로 데이터셋의 어휘사전 인덱싱\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)   # 훈련, 검증, 테스트셋 전처리"
      ],
      "metadata": {
        "id": "uPiQnI3oFKGW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유니그램 데이터셋의 출력 확인하기\n",
        "\n",
        "for inputs, targets in binary_1gram_train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)  # 입력은 20,000차원 벡터 배치\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])  # 0과 1로만 구성\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W4Po_WaHAjk",
        "outputId": "685a2377-fce6-443f-c6d2-8a26ac757787"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성 유틸리티\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "  inputs = keras.Input(shape=(max_tokens,))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\",\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "BmlWKvdIHC52"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이진 유니그램 모델 훈련하고 테스트하기\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.h5\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),  # cache() 메서드를 호출하여 메모리에 캐싱\n",
        "\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.h5\")\n",
        "print(f\"테스트 정확도: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg7yJeO6JYWs",
        "outputId": "71031b15-56cb-405b-89d1-88afa95c535f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 10s 12ms/step - loss: 0.3919 - accuracy: 0.8360 - val_loss: 0.2911 - val_accuracy: 0.8862\n",
            "Epoch 2/10\n",
            " 41/625 [>.............................] - ETA: 2s - loss: 0.2994 - accuracy: 0.8857"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2667 - accuracy: 0.9044 - val_loss: 0.2872 - val_accuracy: 0.8914\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2332 - accuracy: 0.9187 - val_loss: 0.3019 - val_accuracy: 0.8906\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2136 - accuracy: 0.9267 - val_loss: 0.3126 - val_accuracy: 0.8892\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2138 - accuracy: 0.9298 - val_loss: 0.3276 - val_accuracy: 0.8890\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2020 - accuracy: 0.9357 - val_loss: 0.3421 - val_accuracy: 0.8860\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1996 - accuracy: 0.9388 - val_loss: 0.3459 - val_accuracy: 0.8878\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1962 - accuracy: 0.9380 - val_loss: 0.3586 - val_accuracy: 0.8884\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1954 - accuracy: 0.9412 - val_loss: 0.3637 - val_accuracy: 0.8838\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1816 - accuracy: 0.9413 - val_loss: 0.3732 - val_accuracy: 0.8846\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2937 - accuracy: 0.8862\n",
            "테스트 정확도: 0.886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 이진 인코딩을 사용한 바이그램"
      ],
      "metadata": {
        "id": "o5226DsiSxuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 바이그램을 반환하는 TextVectorization 층 만들기\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ],
      "metadata": {
        "id": "vx7Zyz2DKdy7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이진 바이그램 모델 훈련하고 테스트하기\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"테스트 정확도: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62-R1NiKMked",
        "outputId": "2e7de2a1-604c-4c67-9ee5-e07b6f3d6d36"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 7s 10ms/step - loss: 0.3885 - accuracy: 0.8390 - val_loss: 0.2722 - val_accuracy: 0.8940\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2522 - accuracy: 0.9130 - val_loss: 0.2695 - val_accuracy: 0.8974\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2125 - accuracy: 0.9296 - val_loss: 0.2880 - val_accuracy: 0.8948\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1945 - accuracy: 0.9388 - val_loss: 0.3068 - val_accuracy: 0.8946\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1806 - accuracy: 0.9453 - val_loss: 0.3219 - val_accuracy: 0.8924\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1718 - accuracy: 0.9499 - val_loss: 0.3351 - val_accuracy: 0.8920\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1588 - accuracy: 0.9528 - val_loss: 0.3639 - val_accuracy: 0.8908\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1642 - accuracy: 0.9548 - val_loss: 0.3717 - val_accuracy: 0.8874\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1541 - accuracy: 0.9572 - val_loss: 0.3840 - val_accuracy: 0.8804\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1509 - accuracy: 0.9577 - val_loss: 0.4041 - val_accuracy: 0.8804\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2680 - accuracy: 0.8988\n",
            "테스트 정확도: 0.899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF-IDF 인코딩을 사용한 바이그램"
      ],
      "metadata": {
        "id": "7o_P-5ApSm_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰 카운트를 반환하는 TextVectorization 층\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ],
      "metadata": {
        "id": "8Of7u9S8NsS4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF 가중치가 적용된 출력을 반환하는 TextVectorization 층\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ],
      "metadata": {
        "id": "c0hRcFUiTI0j"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF 바이그램 모델 훈련하고 테스트하기\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds) # 어휘 사전과 TF-IDF 가중치 학습\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.h5\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.h5\")\n",
        "print(f\"테스트 정확도: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI3NDqd1VvF2",
        "outputId": "471d551c-bd85-4d1b-aa86-97ebfdf44187"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.4556 - accuracy: 0.8077 - val_loss: 0.2716 - val_accuracy: 0.9000\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2919 - accuracy: 0.8925 - val_loss: 0.2756 - val_accuracy: 0.9014\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2505 - accuracy: 0.9068 - val_loss: 0.3070 - val_accuracy: 0.8914\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2269 - accuracy: 0.9147 - val_loss: 0.3219 - val_accuracy: 0.8946\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2127 - accuracy: 0.9198 - val_loss: 0.3393 - val_accuracy: 0.8868\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2041 - accuracy: 0.9225 - val_loss: 0.3624 - val_accuracy: 0.8912\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1954 - accuracy: 0.9245 - val_loss: 0.3669 - val_accuracy: 0.8810\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1889 - accuracy: 0.9281 - val_loss: 0.3726 - val_accuracy: 0.8852\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1841 - accuracy: 0.9293 - val_loss: 0.3852 - val_accuracy: 0.8818\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1875 - accuracy: 0.9297 - val_loss: 0.3714 - val_accuracy: 0.8784\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2758 - accuracy: 0.8942\n",
            "테스트 정확도: 0.894\n"
          ]
        }
      ]
    }
  ]
}